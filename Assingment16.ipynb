{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d5aaa9",
   "metadata": {},
   "source": [
    "# Decision Tree | Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e654f73",
   "metadata": {},
   "source": [
    "#### Q.1) What is a Decision Tree, and how does it work in the context of classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ef24f",
   "metadata": {},
   "source": [
    "Answer ->\n",
    "\n",
    "Decision tree: \n",
    "A Decision Tree is a supervised machine learning algorithm that is structured like a flowchart or an upside-down tree. It's used for both classification (predicting a category) and regression (predicting a value) tasks.\n",
    "\n",
    "\n",
    "It's one of the most intuitive algorithms because it mimics human decision-making. It breaks down a complex decision into a series of smaller, simpler questions.\n",
    "\n",
    "\n",
    "üå≥ How It Works for Classification\n",
    "In classification, the goal is to predict which class a new piece of data belongs to. A decision tree does this by learning a set of \"if-then\" rules from the data it's trained on.\n",
    "\n",
    "\n",
    "Here‚Äôs the step-by-step process:\n",
    "\n",
    "Start at the Root Node: The tree begins with a single node, called the root node, which represents the entire dataset.\n",
    "\n",
    "Find the Best Split: The algorithm searches for the best feature (e.g., \"Outlook,\" \"Age,\" \"Humidity\") and the best value to split the data on. The \"best\" split is the one that does the best job of separating the data into distinct classes.\n",
    "\n",
    "\n",
    "For example, if predicting whether to play tennis, splitting by \"Outlook\" (Sunny, Overcast, Rain) might be the most informative first question.\n",
    "\n",
    "Create Branches: This split creates new branches, with each branch leading to a new internal node (or decision node). Each internal node represents a new question or test on another feature.\n",
    "\n",
    "\n",
    "Repeat Recursively: The algorithm repeats Step 2 and 3 for each new internal node. It continuously splits the data into smaller and smaller subsets.\n",
    "\n",
    "\n",
    "Reach Leaf Nodes: This process stops when a node is \"pure\" (meaning all data points in it belong to a single class) or when a predefined stopping condition is met (like a maximum tree depth). These final nodes are called leaf nodes.\n",
    "\n",
    "\n",
    "Assign Class Labels: Each leaf node is assigned the class label that is most common among the data points that ended up there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d94413",
   "metadata": {},
   "source": [
    "#### Q.2) Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18c075",
   "metadata": {},
   "source": [
    "Answer ->\n",
    "\n",
    "In a decision tree, both Gini Impurity and Entropy are metrics used to measure the \"impurity\" or \"disorder\" of a node.\n",
    "\n",
    "Think of impurity as how mixed-up the classes are in a single node.\n",
    "\n",
    "A pure node (low impurity) has data from only one class (e.g., 10 \"Spam\" emails, 0 \"Not Spam\"). This is the goal.\n",
    "\n",
    "An impure node (high impurity) has a mix of classes (e.g., 5 \"Spam\" emails, 5 \"Not Spam\"). This is what the tree tries to fix.\n",
    "\n",
    "The algorithm's main job is to find splits that reduce impurity as much as possible.\n",
    "\n",
    "* Gini Impurity: \n",
    "\n",
    "Gini Impurity measures the probability of incorrectly classifying a randomly chosen element in the node, if it were randomly labeled according to the class distribution in that node.\n",
    "\n",
    "Range: For a binary (2-class) problem, the Gini score is between 0 and 0.5.\n",
    "\n",
    "0 (Pure): The node is perfectly pure (e.g., 100% \"Spam\"). The probability of misclassification is 0.8\n",
    "\n",
    "0.5 (Impure): The node is maximally impure (e.g., 50% \"Spam,\" 50% \"Not Spam\").\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$Gini = 1 - \\sum_{i=1}^{C} (p_i)^2$$\n",
    "\n",
    "Where $p_i$ is the probability (or fraction) of class $i$ in the node.Example (10 data points):\n",
    "\n",
    "Pure Node: 10 \"Spam,\" 0 \"Not Spam\"$p_{spam} = 1.0$, $p_{not\\_spam} = 0.0$$Gini = 1 - ( (1.0)^2 + (0.0)^2 ) = 1 - 1 = 0$\n",
    "\n",
    "Impure Node: 5 \"Spam,\" 5 \"Not Spam\"$p_{spam} = 0.5$, $p_{not\\_spam} = 0.5$$Gini = 1 - ( (0.5)^2 + (0.5)^2 ) = 1 - (0.25 + 0.25) = 1 - 0.5 = 0.5$\n",
    "\n",
    "* Entropy:\n",
    "\n",
    "Entropy is a concept from information theory that measures the amount of \"uncertainty\" or \"disorder\" in a set.\n",
    "Range: For a binary (2-class) problem, Entropy is between 0 and 1.\n",
    "\n",
    "0 (Pure): The node is perfectly pure (e.g., 100% \"Spam\").11 There is no uncertainty.\n",
    "\n",
    "1 (Impure): The node is maximally impure (e.g., 50% \"Spam,\" 50% \"Not Spam\"). You have the least information and maximum uncertainty.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)$$\n",
    "\n",
    "Where $p_i$ is the probability of class $i$.\n",
    "\n",
    "Example (10 data points):\n",
    "\n",
    "Pure Node: 10 \"Spam,\" 0 \"Not Spam\"$p_{spam} = 1.0$, $p_{not\\_spam} = 0.0\n",
    "\n",
    "$$Entropy = - [ (1.0 \\cdot \\log_2(1.0)) + (0 \\cdot \\log_2(0)) ] = 0$\n",
    " \n",
    "(Note: $0 \\cdot \\log_2(0)$ is treated as 0)\n",
    "\n",
    "Impure Node: 5 \"Spam,\" 5 \"Not Spam\"$p_{spam} = 0.5$, $p_{not\\_spam} = 0.5\n",
    "\n",
    "$$Entropy = - [ (0.5 \\cdot \\log_2(0.5)) + (0.5 \\cdot \\log_2(0.5)) ] = - [ (0.5 \\cdot -1) + (0.5 \\cdot -1) ] = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4d511",
   "metadata": {},
   "source": [
    "#### Q.3) What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621ecf5",
   "metadata": {},
   "source": [
    "Answer ->\n",
    "\n",
    "Pruning is a technique used to reduce the size of a decision tree by removing sections of the tree that are non-essential, helping to reduce overfitting and improve the model's ability to generalize to new data.\n",
    "\n",
    "The two main types are Pre-Pruning and Post-Pruning, and their core difference is when they stop the tree from growing.\n",
    "\n",
    "-  Pre-Pruning (Early Stopping)\n",
    "This method stops the tree's growth during the training process, before it's fully built.\n",
    "\n",
    "It works by setting a \"stopping rule\" or a limit. If a new split doesn't meet a certain threshold, the algorithm just stops and turns that node into a leaf. Common rules include:\n",
    "\n",
    "Maximum Depth: Stop splitting once the tree reaches a certain number of levels.\n",
    "\n",
    "Minimum Samples Leaf: Stop splitting if a node has fewer than a specified number of data points.\n",
    "\n",
    "Minimum Improvement: Stop splitting if the split doesn't reduce the impurity (Gini or Entropy) by at least a certain amount.\n",
    "\n",
    "Practical Advantage: Efficiency\n",
    "\n",
    "Its main advantage is speed and computational efficiency. By not bothering to build branches that it will likely throw away, pre-pruning saves a significant amount of training time and resources. This is very useful for large datasets.\n",
    "\n",
    "-  Post-Pruning (Trimming)\n",
    "This method allows the tree to grow to its maximum, complex size first‚Äîletting it fully overfit the training data. After the tree is built, it goes back and \"prunes\" (trims) branches that don't add significant predictive power.\n",
    "\n",
    "A common method (like Cost Complexity Pruning) works by checking if removing a whole subtree (turning an internal node into a leaf) actually improves the model's performance on a separate validation dataset. If the simpler, trimmed tree performs better (or just as well), the branch is permanently removed.\n",
    "\n",
    "Practical Advantage: Accuracy\n",
    "\n",
    "Its main advantage is often higher accuracy and better generalization. By letting the tree grow fully, it can see the \"whole picture.\" It avoids a problem called the \"horizon effect,\" where pre-pruning might stop a split that looks weak, even if it would have led to very good, informative splits further down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94056659",
   "metadata": {},
   "source": [
    "#### Q.4) What is Information Gain in Decision Trees, and why is it important for choosing the best split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2951e764",
   "metadata": {},
   "source": [
    "Answer ->\n",
    "\n",
    "Information Gain is the metric a decision tree uses to measure how much \"purity\" it gains (or how much \"uncertainty\" it reduces) by splitting the data on a particular feature.\n",
    "\n",
    "In simple terms, it's the reduction in impurity (which you know as Gini Impurity or Entropy) that a split provides.\n",
    "\n",
    "---- Why It's Important for Choosing the Best Split:\n",
    "\n",
    "A decision tree is a \"greedy\" algorithm, meaning it wants to make the best possible decision at every single step. Information Gain is the tool it uses to do this.\n",
    "\n",
    "Here‚Äôs the process for picking the \"best\" split at any node:\n",
    "\n",
    "Calculate Parent Impurity: First, the algorithm calculates the impurity (e.g., Entropy) of the current, unsplit node. This is its baseline \"messiness.\"\n",
    "\n",
    "Test All Possible Splits: The algorithm then \"previews\" every possible split it could make.\n",
    "\n",
    "For a feature like \"Outlook\" (Sunny, Overcast, Rain), it checks the split on \"Outlook.\"\n",
    "\n",
    "For a feature like \"Temperature\" (> 70¬∞), it checks the split at 70¬∞.\n",
    "\n",
    "Calculate Child Impurity: For each previewed split, it calculates the weighted average impurity of the new child nodes that would be created.\n",
    "\n",
    "Calculate the Gain: It then finds the Information Gain for that specific split using this formula:\n",
    "\n",
    "Information Gain = Impurity(Parent) - Weighted Average Impurity(Children)\n",
    "\n",
    "Choose the Winner: After calculating the Information Gain for all possible splits, the algorithm simply chooses the split that produced the highest Information Gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49ea5d",
   "metadata": {},
   "source": [
    "#### Q.5) What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56e728",
   "metadata": {},
   "source": [
    "Answer ->\n",
    "\n",
    "Common Real-World Applications\n",
    "\n",
    "Here are some of the most common ways decision trees are used:\n",
    "\n",
    "Healthcare (Medical Diagnosis): Doctors can use them as a diagnostic tool. The tree asks a series of questions based on symptoms, lab results, and patient history (e.g., \"Is body temperature > 101¬∞F?\", \"Does the patient have a cough?\") to suggest a potential diagnosis.\n",
    "\n",
    "Finance (Credit Scoring & Fraud Detection):\n",
    "\n",
    "Credit Scoring: Banks use decision trees to determine if a loan applicant is a high or low credit risk. The tree branches on factors like income, age, credit history, and loan amount.\n",
    "\n",
    "Fraud Detection: They can classify financial transactions as \"Legitimate\" or \"Fraudulent\" in real-time by analyzing factors like transaction amount, location, and user's purchase history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5696de",
   "metadata": {},
   "source": [
    "##### Q.6) Question 6: Write a Python program to:\n",
    "‚óè Load the Iris Dataset\n",
    "\n",
    "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
    "\n",
    "‚óè Print the model‚Äôs accuracy and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b1a92",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.14.0' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Python314/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_iris_decision_tree():\n",
    "    \"\"\"\n",
    "    Loads the Iris dataset, trains a Decision Tree Classifier,\n",
    "    and prints the model's accuracy and feature importances.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load the Iris Dataset\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    feature_names = iris.feature_names\n",
    "    \n",
    "    print(\"Dataset loaded successfully.\\n\")\n",
    "\n",
    "    # 2. Split the data into training and testing sets\n",
    "    # We use a 70/30 split and a random_state for reproducibility\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 3. Train a Decision Tree Classifier using the Gini criterion\n",
    "    # We explicitly set criterion='gini' (which is also the default)\n",
    "    # We set random_state for reproducibility of the tree's construction\n",
    "    clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "    \n",
    "    print(\"Training Decision Tree with Gini criterion...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Training complete.\\n\")\n",
    "\n",
    "    # 4. Make predictions on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # 5. Print the model's accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"--- Model Performance ---\")\n",
    "    print(f\"Accuracy on the test set: {accuracy:.4f} (or {accuracy*100:.2f}%)\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # 6. Print the feature importances\n",
    "    # Feature importances show how much each feature contributed to\n",
    "    # reducing the Gini impurity in the tree.\n",
    "    importances = clf.feature_importances_\n",
    "    \n",
    "    # Create a pandas Series for easier viewing\n",
    "    feature_importance_series = pd.Series(\n",
    "        importances, index=feature_names\n",
    "    ).sort_values(ascending=False)\n",
    "\n",
    "    print(\"--- Feature Importances ---\")\n",
    "    print(feature_importance_series)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"Interpretation:\")\n",
    "    print(\"The model's accuracy is perfect on this test split (1.0).\")\n",
    "    print(\"The feature importances show that 'petal width (cm)' and 'petal length (cm)'\")\n",
    "    print(\"were the most important features for making classification decisions.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_iris_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3623798",
   "metadata": {},
   "source": [
    "#### Q.7) Write a Python program to:\n",
    "\n",
    "‚óè Load the Iris Dataset\n",
    "\n",
    "‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a16afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compare_tree_depths():\n",
    "    \"\"\"\n",
    "    Loads the Iris dataset, splits it, trains two Decision Tree\n",
    "    Classifiers (one fully-grown, one with max_depth=3),\n",
    "    and compares their accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load the Iris Dataset\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    \n",
    "    print(\"Dataset loaded successfully.\\n\")\n",
    "\n",
    "    # 2. Split the data into training and testing sets\n",
    "    # We use a 70/30 split.\n",
    "    # We set random_state=42 to ensure that both models\n",
    "    # are trained and tested on the *exact same* data split,\n",
    "    # which is essential for a fair comparison.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"Data split into 70% train (105 samples) and 30% test (45 samples).\\n\")\n",
    "\n",
    "    # --- Model 1: Fully-Grown Decision Tree ---\n",
    "    # We don't specify max_depth, so the tree can grow\n",
    "    # as deep as it needs to, which risks overfitting.\n",
    "    # We set random_state=42 for reproducible results.\n",
    "    clf_full = DecisionTreeClassifier(random_state=42)\n",
    "    \n",
    "    print(\"Training fully-grown tree...\")\n",
    "    clf_full.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the fully-grown tree\n",
    "    y_pred_full = clf_full.predict(X_test)\n",
    "    acc_full = accuracy_score(y_test, y_pred_full)\n",
    "    \n",
    "    print(\"Training complete.\\n\")\n",
    "\n",
    "    # --- Model 2: Pre-Pruned Decision Tree (max_depth=3) ---\n",
    "    # We set max_depth=3, which is a form of pre-pruning.\n",
    "    # This stops the tree from growing past 3 levels.\n",
    "    clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "    \n",
    "    print(\"Training pre-pruned (max_depth=3) tree...\")\n",
    "    clf_pruned.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the pruned tree\n",
    "    y_pred_pruned = clf_pruned.predict(X_test)\n",
    "    acc_pruned = accuracy_score(y_test, y_pred_pruned)\n",
    "    \n",
    "    print(\"Training complete.\\n\")\n",
    "    \n",
    "    # --- 4. Print the Comparison ---\n",
    "    print(\"--- Model Accuracy Comparison ---\")\n",
    "    print(f\"Fully-Grown Tree Depth: {clf_full.get_depth()} levels\")\n",
    "    print(f\"Fully-Grown Tree Accuracy: {acc_full:.4f} (or {acc_full*100:.2f}%)\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Pre-Pruned Tree Depth: {clf_pruned.get_depth()} levels\")\n",
    "    print(f\"Pre-Pruned (max_depth=3) Tree Accuracy: {acc_pruned:.4f} (or {acc_pruned*100:.2f}%)\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"--- Conclusion ---\")\n",
    "    if acc_full == acc_pruned:\n",
    "        print(\"In this specific case, both models achieved the same (perfect) accuracy.\")\n",
    "        print(\"This is common on the simple Iris dataset.\")\n",
    "        print(\"The max_depth=3 tree is simpler and just as effective, making it the better model.\")\n",
    "    elif acc_pruned > acc_full:\n",
    "        print(\"The pre-pruned (max_depth=3) tree performed *better*.\")\n",
    "        print(\"This suggests the fully-grown tree was overfitting the training data.\")\n",
    "    else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc7b01",
   "metadata": {},
   "source": [
    "#### Q.8) Write a Python program to:\n",
    "\n",
    "‚óè Load the Boston Housing Dataset\n",
    "\n",
    "‚óè Train a Decision Tree Regressor\n",
    "\n",
    "‚óè Print the Mean Squared Error (MSE) and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing # Using California Housing as Boston is deprecated\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_housing_regressor():\n",
    "    \"\"\"\n",
    "    Loads the California Housing dataset (in place of Boston Housing),\n",
    "    trains a Decision Tree Regressor, and prints the model's\n",
    "    Mean Squared Error (MSE) and feature importances.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load the Dataset\n",
    "    # Note: load_boston() is deprecated and removed from scikit-learn.\n",
    "    # We are using fetch_california_housing() as the modern alternative.\n",
    "    housing = fetch_california_housing()\n",
    "    X = housing.data\n",
    "    y = housing.target\n",
    "    feature_names = housing.feature_names\n",
    "    \n",
    "    print(\"Loaded California Housing dataset (as Boston Housing is deprecated/removed).\\n\")\n",
    "\n",
    "    # 2. Split the data into training and testing sets\n",
    "    # This is crucial for getting a meaningful MSE on unseen data.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Split data into {len(X_train)} training samples and {len(X_test)} test samples.\\n\")\n",
    "\n",
    "    # 3. Train a Decision Tree Regressor\n",
    "    # We set random_state=42 for reproducible results\n",
    "    regressor = DecisionTreeRegressor(random_state=42)\n",
    "    \n",
    "    print(\"Training Decision Tree Regressor...\")\n",
    "    regressor.fit(X_train, y_train)\n",
    "    print(\"Training complete.\\n\")\n",
    "\n",
    "    # 4. Make predictions on the test set\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    # 5. Print the Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse) # Get Root Mean Squared Error for easier interpretation\n",
    "\n",
    "    print(\"--- Model Performance ---\")\n",
    "    print(f\"Mean Squared Error (MSE) on test set: {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE) on test set: {rmse:.4f}\")\n",
    "    print(\"(Note: RMSE is in the same unit as the target, $100,000s)\\n\")\n",
    "\n",
    "    # 6. Print the feature importances\n",
    "    importances = regressor.feature_importances_\n",
    "    \n",
    "    # Create a pandas Series for easier viewing, sorted by importance\n",
    "    feature_importance_series = pd.Series(\n",
    "        importances, index=feature_names\n",
    "    ).sort_values(ascending=False)\n",
    "\n",
    "    print(\"--- Feature Importances ---\")\n",
    "    print(feature_importance_series)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Interpretation:\")\n",
    "    print(\"The feature importances show which features (like MedInc - median income)\")\n",
    "    print(\"were the most decisive for predicting the housing price.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_housing_regressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2c6e3",
   "metadata": {},
   "source": [
    "#### Q.9)  Write a Python program to:\n",
    "\n",
    "‚óè Load the Iris Dataset\n",
    "\n",
    "‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using GridSearchCV\n",
    "\n",
    "‚óè Print the best parameters and the resulting model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c2f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def tune_iris_decision_tree():\n",
    "    \"\"\"\n",
    "    Loads the Iris dataset, tunes a Decision Tree's hyperparameters\n",
    "    using GridSearchCV, and prints the best parameters and final accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load the Iris Dataset\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    \n",
    "    print(\"Dataset loaded successfully.\\n\")\n",
    "\n",
    "    # 2. Split the data into training and testing sets\n",
    "    # We use the training set for tuning and the test set\n",
    "    # for a final, unbiased evaluation.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Split data into {len(X_train)} training samples and {len(X_test)} test samples.\\n\")\n",
    "\n",
    "    # 3. Define the parameter grid to search\n",
    "    # This grid includes different values to try for\n",
    "    # 'max_depth' and 'min_samples_split'.\n",
    "    param_grid = {\n",
    "        'max_depth': [2, 3, 4, 5, None],  # 'None' means the tree grows fully\n",
    "        'min_samples_split': [2, 5, 10, 15]\n",
    "    }\n",
    "    \n",
    "    print(\"Parameter grid to search:\")\n",
    "    print(param_grid)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # 4. Initialize the Decision Tree and GridSearchCV\n",
    "    # We use a base DecisionTreeClassifier (random_state for reproducibility)\n",
    "    # and wrap it in GridSearchCV.\n",
    "    dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "    \n",
    "    # cv=5 means 5-fold cross-validation\n",
    "    # scoring='accuracy' means we optimize for accuracy\n",
    "    # n_jobs=-1 uses all available CPU cores to speed up the search\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=dt_classifier,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # 5. Run the Grid Search on the training data\n",
    "    print(\"Running GridSearchCV... (This may take a moment)\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"Tuning complete.\\n\")\n",
    "\n",
    "    # 6. Print the best parameters found\n",
    "    print(\"--- Best Hyperparameters Found ---\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # 7. Print the resulting model accuracy\n",
    "    # GridSearchCV automatically finds the best model and re-trains\n",
    "    # it on the *entire* training set. We can access this\n",
    "    # model with grid_search.best_estimator_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # We now evaluate this single best model on our *test set*\n",
    "    # to get a final, unbiased performance score.\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    final_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # You can also see the best cross-validation score from the training phase\n",
    "    best_cv_score = grid_search.best_score_\n",
    "    \n",
    "    print(\"--- Model Performance ---\")\n",
    "    print(f\"Best cross-validation accuracy (on training data): {best_cv_score:.4f}\")\n",
    "    print(f\"Final accuracy on the test set: {final_accuracy:.4f} (or {final_accuracy*100:.2f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tune_iris_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52eb874",
   "metadata": {},
   "source": [
    "#### Q.10) Imagine you‚Äôre working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
    "\n",
    "‚óè Handle the missing values\n",
    "\n",
    "‚óè Encode the categorical features\n",
    "\n",
    "‚óè Train a Decision Tree model\n",
    "\n",
    "‚óè Tune its hyperparameters\n",
    "\n",
    "‚óè Evaluate its performance\n",
    "\n",
    "And describe what business value this model could provide in the real-world setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3fc0db",
   "metadata": {},
   "source": [
    "Answer ->\n",
    "\n",
    "Project Plan: Patient Disease Prediction Model\n",
    "Objective: To develop a reliable machine learning model to predict the presence or absence of a specific disease in patients, using a large dataset with mixed data types and missing values. We will use a Decision Tree classifier as the core algorithm.\n",
    "\n",
    "Step 1: Data Preprocessing (Handling Missing Values & Encoding)\n",
    "The raw dataset cannot be fed directly into a model. We must first clean and transform it.\n",
    "\n",
    "Handle Missing Values:\n",
    "\n",
    "Numerical Features (e.g., Age, Blood_Pressure, Cholesterol):\n",
    "\n",
    "Strategy: Impute using the Median. We choose the median over the mean because it is robust to outliers (e.g., a few extremely high blood pressure readings won't skew the fill value).\n",
    "\n",
    "Example: If 5% of Cholesterol readings are missing, we will calculate the median cholesterol of the entire dataset and fill the missing spots with that value.\n",
    "\n",
    "Categorical Features (e.g., Blood_Type, Symptom_Severity):\n",
    "\n",
    "Strategy: Impute using the Mode (the most frequent value).\n",
    "\n",
    "Alternative: If the \"missing-ness\" itself is predictive (e.g., a \"missing\" test result means the test was never ordered), we will create a new category called 'Unknown' or 'Missing'. This treats the absence of data as its own feature.\n",
    "\n",
    "Encode Categorical Features: The DecisionTreeClassifier in scikit-learn requires all inputs to be numeric. We will encode our categorical features as follows:\n",
    "\n",
    "Nominal Features (no inherent order):\n",
    "\n",
    "Features: Gender, Blood_Type.\n",
    "\n",
    "Method: One-Hot Encoding. This creates new binary (0/1) columns for each category. For example, Gender would be split into two columns: Gender_Male and Gender_Female. This prevents the model from incorrectly assuming that one category is \"greater than\" another.\n",
    "\n",
    "Ordinal Features (a clear order exists):\n",
    "\n",
    "Features: Symptom_Severity (e.g., 'Low', 'Medium', 'High').\n",
    "\n",
    "Method: Ordinal Encoding (or Label Encoding). We will map these to integers that preserve their order, such as Low=0, Medium=1, High=2.\n",
    "\n",
    "Step 2: Train a Baseline Decision Tree Model\n",
    "Before we tune, we need a baseline to know if our efforts are working.\n",
    "\n",
    "Split the Data: We will split our preprocessed data into two sets:\n",
    "\n",
    "80% Training Set: Used to train the model and for hyperparameter tuning.\n",
    "\n",
    "20% Test Set: \"Locked away\" and used only once at the very end for an unbiased evaluation.\n",
    "\n",
    "Train Baseline Model: We will train a DecisionTreeClassifier on the training set with all its default parameters. This tree will almost certainly be overfit (i.e., it will grow to be perfectly accurate on the training data but will perform poorly on new data).\n",
    "\n",
    "Step 3: Tune Hyperparameters with GridSearchCV\n",
    "Our goal is to prevent overfitting by \"pruning\" the tree. We will use GridSearchCV to test all combinations of key parameters and find the best-performing set.\n",
    "\n",
    "Define Parameter Grid: We will create a \"grid\" of parameters to test:\n",
    "\n",
    "criterion: ['gini', 'entropy'] (The two impurity measures).\n",
    "\n",
    "max_depth: [3, 5, 7, 10] (Controls how deep the tree can grow).\n",
    "\n",
    "min_samples_split: [10, 20, 40] (The minimum number of patients in a node required to split it further).\n",
    "\n",
    "min_samples_leaf: [5, 10, 20] (The minimum number of patients allowed in a final \"leaf\" node).\n",
    "\n",
    "Run Grid Search: GridSearchCV will use 5-fold cross-validation on our training set. It will automatically:\n",
    "\n",
    "Split the 80% training set into 5 \"folds.\"\n",
    "\n",
    "Train on 4 folds and validate on the 5th, for every single parameter combination.\n",
    "\n",
    "Rotate this process 5 times.\n",
    "\n",
    "Identify the parameter combination (e.g., max_depth=5, min_samples_leaf=10) that had the best average performance.\n",
    "\n",
    "Get Best Model: The output will be our \"best\" tuned model, trained and ready.\n",
    "\n",
    "Step 4: Evaluate Model Performance (The Right Way)\n",
    "This is the most critical step, especially in healthcare. A simple accuracy score is not enough.\n",
    "\n",
    "Use the Test Set: We will take our \"best\" tuned model from Step 3 and make predictions on the 20% test set (which the model has never seen).\n",
    "\n",
    "Generate a Confusion Matrix: This is our primary tool.\n",
    "\n",
    "True Positives (TP): Model predicted \"Disease,\" and the patient has it. (Good)\n",
    "\n",
    "True Negatives (TN): Model predicted \"No Disease,\" and the patient doesn't have it. (Good)\n",
    "\n",
    "False Positives (FP): Model predicted \"Disease,\" but the patient doesn't have it. (Type I Error)\n",
    "\n",
    "False Negatives (FN): Model predicted \"No Disease,\" but the patient has it. (Type II Error)\n",
    "\n",
    "Analyze Key Metrics:\n",
    "\n",
    "Accuracy: (TP+TN) / Total. The percentage of correct predictions.\n",
    "\n",
    "Precision: TP / (TP+FP). \"Of all patients we predicted have the disease, how many actually do?\" High precision reduces unnecessary panic and follow-up tests (lowers FPs).\n",
    "\n",
    "Recall (Sensitivity): TP / (TP+FN). \"Of all patients who actually have the disease, how many did we find?\" This is our most important metric. A \"False Negative\" (a sick patient told they are healthy) is the worst possible outcome in diagnostics. We must maximize Recall.\n",
    "\n",
    "F1-Score: The harmonic mean of Precision and Recall. A great single score for balancing both.\n",
    "\n",
    "ROC-AUC Curve: A plot showing how well our model can distinguish between the two classes.\n",
    "\n",
    "Real-World Business Value\n",
    "This model, once properly validated, provides immense value beyond a simple prediction.\n",
    "\n",
    "Improved Patient Outcomes (Clinical Value):\n",
    "\n",
    "Early Detection: The model can act as an early warning system, flagging at-risk patients before their symptoms become severe. This directly leads to better prognoses and saved lives.\n",
    "\n",
    "Reducing False Negatives: Our focus on maximizing Recall means the model is optimized to \"catch\" as many sick patients as possible, reducing the chance that a person with the disease is mistakenly sent home.\n",
    "\n",
    "Operational Efficiency (Decision Support):\n",
    "\n",
    "Patient Triage: The model provides a risk score that helps clinicians prioritize. A patient with a 95% predicted risk can be fast-tracked to a specialist, while a 5% risk patient can be scheduled for a routine check-up.\n",
    "\n",
    "Resource Allocation: Helps hospitals allocate limited resources (like specific diagnostic machines or specialist time) to the patients who need them most.\n",
    "\n",
    "Cost Reduction (Financial Value):\n",
    "\n",
    "Optimizing Diagnostics: By managing Precision, the model helps reduce the number of \"False Positives.\" This saves the healthcare system (and the patient) money by avoiding expensive, invasive, and unnecessary follow-up tests on healthy individuals.\n",
    "\n",
    "Research & Discovery (Strategic Value):\n",
    "\n",
    "Explainability: A key advantage of Decision Trees is that we can print their feature_importances_. We can see exactly which factors the model found most predictive (e.g., Cholesterol > 200, Blood_Pressure > 140, Blood_Type = 'A'). This can provide new medical insights and guide future research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d88045",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
